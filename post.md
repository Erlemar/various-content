

https://digits-draw-recognize.herokuapp.com/

Эта статья описывает проект по распознаванию рукописного ввода цифр. Вы сможете узнать, как сделать сделать следующее практически с нуля следующее:

- создать простой сайт с использованием Flask и Bootstrap;

- разместить его на платформе Heroku;

- реализовать сохранение и загрузку данных на облаке Amazon s3;

- собрать собственный датасет;

- натренировать модели машинного обучения (FNN и CNN);

- сделать возможность дообучения этих моделей;

- объединить всё это в работающий сайт;

Для полного понимания проекта необходимо знать как работает deep learning для распознавания изображений, иметь базовые знания о Flask и немного разбираться в HTML, JS и CSS.



## Немного обо мне

Я лишь около года занимаюсь машинным обучением и всем связанным с этим (до этого 4 года работал в консалтинге на внедрении ERP-систем). Для получения и развития навыков решил делать такие вот проекты, чтобы глубже изучать интересные мне темы, получать практические навыки и в результате создавать полноценные сайты, которые всегда можно запустить и показать интересующимся.


## Зарождение идеи

Несколько месяцев назад я прошёл специализацию Яндекса и МФТИ на Coursera. У неё есть свой team в Slack, и в апреле там самоорганизовалась группа для прохождения Стенфордского курса cs231n. Как это происходило - отдельная история, но ближе к делу. Важной частью курса является самостоятельный проект (40% оценки для студентов), в котором предоставляется полная свобода действий. Я не хотел делать что-то серьёзное и потом писать про это статью ибо не видел в этом смысла, но всё же душа просила сделать что-то для достойного завершения курса. Примерно в это мне на глаза попался сайт https://tensorflow-mnist.herokuapp.com/ где можно нарисовать цифру и 2 сетки на Tensorflow мгновенно распознают её и покажут результат.



# Основная часть
Здесь я расскажу о том, что и как я делал, чтобы реализовать проект. Объяснения будут достаточно подробными, чтобы было можно их повторить, но некоторые совсем базовые вещи я буду описывать кратко или пропускать.


## Планирование проекта
Перед тем как приступать к чему-то большому стоит это что-то распланировать. По ходу дела будут выясняться новые подробности и план придётся скорректировать, но некое изначальное видение просто обязано быть.
1. Для любого проекта по машинному обучению одним из основополагающих моментов является вопрос о том, какие данные использовать и где их взять. Датасет MNIST активно используется в задачах распознавация цифр, и именно поэтому я не захотел его использовать. В интернете можно найти примеры подобных проектов, где модели натренированы на MNIST (например, https://github.com/sugyan/tensorflow-mnist), мне же хотелось сделать что-то новое. И, наконец, мне казалось, что многие из цифр в MNIST далеки от реальности - при просмотре датасета я встречал много таких вариантов, которые сложно представить в реальности (если только у человека совсем уж жуткий почерк). Плюс рисование цифр в браузере мышкой несколько отличается от их написания ручкой. Как следствие я решил собрать собственный датасет;
2. Следующий (а точнее одновременный) шаг - это создание сайта для сбора данных, а в дальнейшем - создание нового сайта так, чтобы он мог давать предсказания. На тот момент у меня имелись базовые знания Flask, а также HTML, JS и CSS. Поэтому я и решил делать сайт на Flask, а в качестве хостинга была выбрана платформа Heroku, как позволяющая быстро и просто захостить маленький сайт;
3. Далее предстояло создать сами модели, которые должны делать основную работу. Этот этап казался самым простым, поскольку после cs231n имелся достаточный опыт создания архитектуры нейронных сетей для распознавания изображений. Предварительно хотелось сделать несколько архитектур, но в дальнейшем решил остановиться на двух - FNN и CNN. Кроме того, нужно было сделать возможность дотренировки этих моделей, и некоторые идеи на этот счёт у меня уже были;
4. После подготовки моделей следует придать сайту приличный вид, как-то отображать предсказания, дать возможность оценивать корректность ответа, немного описать функционал и сделать ряд других мелких и не очень вещей. На этапе планирования я не стал уделять много времени на размышления об этом, просто составил список;

## Сбор данных
На сбор данных у меня ушла чуть ли не половина всего времени, потраченного на проект. Дело в том, что я слабо был знаком с тем что надо было сделать, поэтому приходилось двигаться методом проб и ошибок. Естественно, сложным было не само рисование, а создание сайта, на котором можно было бы рисовать цифры и куда-то сохранять их. Для этого мне потребовалось получше узнать Flask, поковыряться в Javascript, познакомиться с облаком Amazon S3 и узнать о Heroku. Всё это я описываю достаточно подробно, чтобы можно было это повторить имея такой же уровень знаний, который был у меня на начало проекта.

Предварительно я нарисовал вот такую схему, которая сочетает описание происходящего и технические моменты:

![](https://raw.githubusercontent.com/Erlemar/various-content/master/10.jpg?raw=true)

Сам сбор данных занял несколько дней, ну или несколько часов чистого времени. Я нарисовал 1000 цифр, примерно по 100 каждой (но не точно), пытался рисовать разными стилями, но, конечно, не мог охватить все возможные варианты почерка, но это и не было целью.

### Создание первой версии сайта (для сбора данных)
Первый вариант сайта выглядел вот так: 

![](https://raw.githubusercontent.com/Erlemar/various-content/master/1.jpg?raw=true)

В нём была только самая базовая функциональность:

* канвас для рисования;
* радио-кнопки для выбора лейбла;
* кнопки для сохранения картинок и очистки канваса;
* поле, в котором писалось успешно ли было сохранение;
* сохранение картинок на облаке Amazon;

Итак, теперь подробнее обо всём этом. Специально для статьи я сделал минимально рабочую версию сайта, на примере которой и буду рассказывать, как сделать вышеперечисленное: https://digits-little.herokuapp.com/ [Код](https://github.com/Erlemar/digits_little)

### Flask
Flask - питоновский фреймворк для создания сайтов. На официальном сайте есть отличное [введение](http://flask.pocoo.org/docs/0.12/quickstart/#quickstart). Есть разные способы использования Flask для получения и передачи информации, так в этом проекте я использовал AJAX. AJAX даёт возможность "фонового" обмена данными между браузером и веб-сервером, это позволяет не перезагружать страницы каждый раз при передаче данных.

### Структура проекта

![](https://raw.githubusercontent.com/Erlemar/various-content/master/9.jpg?raw=true)

Все файлы, используемые в проекте можно разделить на 2 неравные группы: меньшая часть необходима для того, чтобы приложение могло работать на Heroku, а все остальные задействованы непосредственно в работе сайта.

### HTML и JS
HTML-файлы должны храниться в папке "template", на данной стадии было достаточно иметь один.

```html
<!doctype html>
<html>
<head>
	<meta charset="utf-8" />
	<title>Handwritten digit recognition</title>
	<link rel="stylesheet" type="text/css" href="static/bootstrap.min.css">
	<link rel="stylesheet" type="text/css" href="static/style.css">
</head>

	<body>

		<div  class="container">
			<div>
				Здесь можно порисовать.<br>
				<canvas id="the_stage" width="200" height="200">fsf</canvas>
				<div>
					<button type="button" class="btn btn-default butt" onclick="clearCanvas()"><strong>clear</strong></button>
					<button type="button" class="btn btn-default butt" id="save" onclick="saveImg()"><strong>save</strong></button>
				</div>

				<div>
					Please select one of the following<br>
					<input type="radio" name="action" value="0" id="digit">0<br>
					<input type="radio" name="action" value="1" id="digit">1<br>
					<input type="radio" name="action" value="2" id="digit">2<br>
					<input type="radio" name="action" value="3" id="digit">3<br>
					<input type="radio" name="action" value="4" id="digit">4<br>
					<input type="radio" name="action" value="5" id="digit">5<br>
					<input type="radio" name="action" value="6" id="digit">6<br>
					<input type="radio" name="action" value="7" id="digit">7<br>
					<input type="radio" name="action" value="8" id="digit">8<br>
					<input type="radio" name="action" value="9" id="digit">9<br>
				</div>
			</div>

			<div class="col-md-6 column">
				<h3>result:</h3>
				<h2 id="rec_result"></h2>
			</div>
		</div>
		<script src="static/jquery.min.js"></script>
		<script src="static/bootstrap.min.js"></script>
		<script src="static/draw.js"></script>
	</body>
</html>
```

В шапке документа и в конце тега body находятся ссылки на файлы js и css. Сами эти файлы должны находиться в папке "static". Теперь подробнее о том, как работает рисование на canvas и сохранение рисунка.
Canvas - это двухмерный элемент HTML5 для рисования. Изображение может рисоваться скриптом или пользователь может иметь возможность рисовать, используя мышь (или касаясь сенсорного экрана).
Canvas задаётся в HTML следующим образом:
```html
<canvas id="the_stage" width="200" height="200"> </canvas>
```

До этого я не был знаком с этим элементом HTML, поэтому мои изначальные попытки сделать возможность рисования были неудачными. Через некоторое время я нашёл работающий пример и позаимствовал его (ссылка есть в моём файле draw.js).

```javascript
var drawing = false;
var context;
var offset_left = 0;
var offset_top = 0;

function start_canvas () {
    var canvas = document.getElementById ("the_stage");
    context = canvas.getContext ("2d");
    canvas.onmousedown = function (event) {mousedown(event)};
    canvas.onmousemove = function (event) {mousemove(event)};
    canvas.onmouseup   = function (event) {mouseup(event)};
    for (var o = canvas; o ; o = o.offsetParent) {
    offset_left += (o.offsetLeft - o.scrollLeft);
    offset_top  += (o.offsetTop - o.scrollTop);
    }
    draw();
}

function getPosition(evt) {
    evt = (evt) ?  evt : ((event) ? event : null);
    var left = 0;
    var top = 0;
    var canvas = document.getElementById("the_stage");

    if (evt.pageX) {
    left = evt.pageX;
    top  = evt.pageY;
    } else if (document.documentElement.scrollLeft) {
    left = evt.clientX + document.documentElement.scrollLeft;
    top  = evt.clientY + document.documentElement.scrollTop;
    } else  {
    left = evt.clientX + document.body.scrollLeft;
    top  = evt.clientY + document.body.scrollTop;
    }
    left -= offset_left;
    top -= offset_top;

    return {x : left, y : top}; 
}

function
mousedown(event) {
    drawing = true;
    var location = getPosition(event);
    context.lineWidth = 8.0;
    context.strokeStyle="#000000";
    context.beginPath();
    context.moveTo(location.x,location.y);
}

function
mousemove(event) {
    if (!drawing) 
        return;
    var location = getPosition(event);
    context.lineTo(location.x,location.y);
    context.stroke();
}

function
mouseup(event) {
    if (!drawing) 
        return;
    mousemove(event);
	context.closePath();
    drawing = false;
}

.
.
.
onload = start_canvas;
```
При загрузке страницы сразу запускается функция start_canvas. Первые две строчки находят канвас как элемент с определенным id ("the stage") и определяют его как двухмерное изображение.
При рисовании на canvas есть 3 события: onmousedown, onmousemove и onmouseup. Есть ещё аналогичные события для касаний, но об этом позже.

onmousedown - происходит при клике на canvas. В этот момент задается ширина и цвет линии, а также определяется начальная точка рисования. На словах определение местоположения курсора звучит просто, но по факту это не совсем тривиально. Для нахождения точки используется функция **getPosition()** - она находит координаты курсора на странице и определяет координаты точки на canvas с учетом относительного положения canvas на странице и с учетом того, что страница может быть проскроллена. После нахождения точки **context.beginPath()** начинает путь рисования, а **context.moveTo(location.x,location.y)** "передвигает" этот путь к точке, которая была определена в момент клика.

onmousemove - следование за движением мышки при нажатой левой клавише. В самом начале сделана проверка на то, что клавиша нажата (то есть drawing = true), если же нет - рисование не осуществляется. **context.lineTo()** создаёт линию по траектории движения мыши, а **context.stroke()** непосредственно рисует её.

mouseup - происходит при отпускании левой клавиши мыши. **context.closePath()** завершает рисование линии.

Вот так и осуществляется рисование на canvas. В интерфейсе есть ещё 4 элемента:

* "Поле" с текущим статусом. JS обращается к нему по id (rec_result) и отображает текущий статус. Статус либо пуст, либо показывает, что изображение сохраненяется, либо показывает название сохранённого изображения.
```html
            <div class="col-md-6 column">
                <h3>result:</h3>
                <h2 id="rec_result"></h2>
            </div>
```

* Радио-кнопки для выбора цифры. На этапе сбора данных нарисованным цифрам нужно как-то присваивать лейблы, для этого и были добавлены 10 кнопок. Кнопки задаются одинаковым способом: `<input type="radio" name="action" value="0" id="digit">0<br>`, где на месте 0 стоит соответствующая цифра. *Name* используется для того, чтобы JS мог получить значение активной радио-кнопки (*value*);
* Кнопка для очищения canvas - чтобы можно было нарисовать новую цифру. `<button type="button" class="btn btn-default butt" id="save" onclick="saveImg()"><strong>save</strong></button>` При нажатии на эту кнопку происходит следующее:

```javascript
function draw() {
    context.fillStyle = '#ffffff';
    context.fillRect(0, 0, 200, 200);
}

function clearCanvas() {
    context.clearRect (0, 0, 200, 200);
    draw();
    document.getElementById("rec_result").innerHTML = "";
}
```

Содержимое canvas очищается, и он заливается белым цветом. Также статус становится пустым.

* Наконец, кнопка сохранения нарисованного изображения. Она вызывает следующую функцию Javascript:

```javascript
	function saveImg() {
	document.getElementById("rec_result").innerHTML = "connecting...";
	var canvas = document.getElementById("the_stage");
	var dataURL = canvas.toDataURL('image/jpg');
	var dig = document.querySelector('input[name="action"]:checked').value;
	$.ajax({
	  type: "POST",
	  url: "/hook",
	  data:{
		imageBase64: dataURL,
		digit: dig
		}
	}).done(function(response) {
	  console.log(response)
	  document.getElementById("rec_result").innerHTML = response
	});
	
}
```
Сразу же после нажатия кнопки в поле статуса отбражается значение "connecting...". Затем изображение конвертируется в текстовую строку с помощью метода кодирования base64. Результат выглядит следующим образом: `"data:image/png;base64,%string%`, где можно увидеть тип файла (image), расширение (png), кодирование base64 и сам стринг. Тут хочу заметить, что я слишком поздно заметил ошибку в моём коде. Мне следовало использовать 'image/jpeg' как аргумент для `canvas.toDataURL()`, но я сделал опечатку и в итоге изображения по факту были png.
Далее я беру значение активной радио-кнопки (по name='action' и по состоянию `checked`) и сохраняю в переменную `dig`.

Наконец, AJAX запрос отправляет закодированное изображение и лейбл в питон, а затем получает ответ. Я довольно много времени потратил на то, чтобы заставить работать эту конструкцию, постараюсь объяснить, что происходит в каждой строке.
Вначале указывается тип запроса - в данном случае "POST", то есть данные из JS передаются в python скрипт.
"/hook" - это куда передаются данные. Поскольку я использую Flask, то я могу в нужном декораторе указать "/hook" в качестве URL, и это будет означать, что именно функция в этом декораторе будет использоваться, когда запрос POST идут на этот URL. Подробнее об этом в разделе про Flask ниже.
data - это данные, которые передаются в запросе. Вариантов передачи данных много, я задаю значение и имя через которое можно получить это значение.
Наконец, `done()` - это то, что происходит при успешном выполнении запроса. Мой AJAX запрос возвращает некий ответ (а точнее текст с именем сохраненного изображения), этот ответ вначале выводится в консоль (для отладки), а затем отображается в поле статуса.

### Flask и сохранение изображения
Теперь перейдём уже к тому, как данные из AJAX запроса попадают в python, и как изображение сохраняется.
Основной скрипт - **main.py**.

Гайдов и статей по работе с Flask полно, поэтому я просто кратко опишу базовые вещи, уделю особое внимание строчкам, без которых код не рабтает, и, конечно, расскажу про остальной код, являющийся основой моего сайта.

**main.py**
```python

__author__ = 'Artgor'
from functions import Model
from flask import Flask, render_template, request
from flask_cors import CORS, cross_origin
import base64
import os

app = Flask(__name__)
model = Model()
CORS(app, headers=['Content-Type'])

@app.route("/", methods=["POST", "GET", 'OPTIONS'])
def index_page():

	return render_template('index.html')

@app.route('/hook', methods = ["GET", "POST", 'OPTIONS'])
def get_image():
	if request.method == 'POST':
		image_b64 = request.values['imageBase64']
		drawn_digit = request.values['digit']
		image_encoded = image_b64.split(',')[1]
		image = base64.decodebytes(image_encoded.encode('utf-8'))		
		save = model.save_image(drawn_digit, image)	

		print('Done')
	return save

if __name__ == '__main__':
	port = int(os.environ.get("PORT", 5000))
	app.run(host='0.0.0.0', port=port, debug=False)
```
**functions.py**
```python
__author__ = 'Artgor'
from codecs import open
import os
import uuid
import boto3
from boto.s3.key import Key
from boto.s3.connection import S3Connection


class Model(object):
	def __init__(self):
		self.nothing = 0

	def save_image(self, drawn_digit, image):
		filename = 'digit' + str(drawn_digit) + '__' + str(uuid.uuid1()) + '.jpg'
		with open('tmp/' + filename, 'wb') as f:
			f.write(image)
			
		print('Image written')
		
		REGION_HOST = 's3-external-1.amazonaws.com'
		conn = S3Connection(os.environ['AWS_ACCESS_KEY_ID'], os.environ['AWS_SECRET_ACCESS_KEY'], host=REGION_HOST)
		bucket = conn.get_bucket('testddr')
		
		k = Key(bucket)
		fn = 'tmp/' + filename
		k.key = filename
		k.set_contents_from_filename(fn)
		print('Done')

		return ('Image saved successfully with the name {0}'.format(filename))
```

Первым делом необходимо создать экземпляр Flask класса с дефолтным значением `app = Flask(__name__)`. Это будет основой приложения.
Далее я создаю экземпляр класса для использования второго скрипта. Учитывая, что в нём всего одна функция, можно было бы обойтись без использования классов или вообще держать весь код в одном скрипте. Но я знал, что количество методов будет расти, поэтому решил сразу использовать такой вариант.

CORS (Cross-origin resource sharing) - технология, предоставляющая веб-страницам доступ к ресурсам другого домена. В данном приложении это используется для того, чтобы сохранять изображения на облаке Amazon. Я долго искал способ активировать это, а потом как сделать это проще всего. В итоге это было реализовано одной строчкой: `CORS(app, headers=['Content-Type'])`.

Далее используется декоратор `route()` - он определяет какая функция выполняется для какого URL. В основном скрипте есть 2 декоратора с функциями. Первый из них используется для основной страницы (поскольку указан адрес `"/"`) и отображает "index.html". Второй же декоратор имеет адрес '"/hook"', а это значит, что именно в него передаются данные из JS (напомню, что там был указан такой же адрес).

Замечу, что оба декоратора в качестве значения параметра method имеют список "POST", "GET", 'OPTIONS'. POST и GET используются для получения и передачи данных, OPTIONS необходим для передачи параметров, таких как CORS. По идее OPTIONS должен использоваться по умолчанию, начиная с версии Flask 0.6, но без его явного указания мне не удалось заставить код заработать.

Теперь поговорим о функции для получения изображения. Из JS в Python приходит 'request'. Это могут быть данные из формы, данные, полученные из AJAX или что-то другое. В моём случае это словарь с ключами и значениями, заданными в JS. Извлечение лейбла картинки тривиально, нужно просто взять значение словаря по ключу, а вот стринг с изображением необходимо обработать - взять часть, относящуюся к самому изображению (отбросив описание) и декодировать.

После этого вызывается функция из второго скрипта для сохранения изображения. Она возвращает стринг с названием сохраненного избражения, который после этого возвращается в JS и отображается на станице браузера.

Последняя часть кода необходима для работы приложения на heroku (взято из документации). Подробнее о том, как разместить приложение на Heroku ниже (тут ссылка).

Наконец, можно посмотреть, как именно сохраняется изображение. На текущий момент изображение хранится в переменной, но для сохранения изображения на облаке Amazon необходимо иметь файл, так что картинку необходимо сохранить. Название картинки содержит её лейбл и уникальный id, генерящийся с помощью uuid). После этого картинка сохраняется во временную папку tmp и заливается на облако. Забегая вперёд скажу, что эта папка воистину временная: heroku хранит файлы в этой папке в течение сессии, а по её завершении очищает папку. Это позволяет не думать о необходимости очищения папки и вообще удобно, но именно из-за этого и приходится использовать облако. А теперь можно поговорить о том, как работает облако Amazon и что можно с ним делать.

### Интеграция с Amazon s3
Amazon s3 - это облако, с которым удобно работать как в интерфейсе, так и с помощью кода. В python есть 2 библиотеки для работы с ним: boto и boto3. Вторая - поновее и лучше поддерживается, но пока некоторые вещи удобнее делать, используя первую. Думаю, что регистрация аккаунта не вызовет никаких проблем. Важно не забыть сгенерить ключи для доступа к облаку с помощью библиотек (Access Key ID and Secret Access Key). В самом облаке можно создавать buckets, где и будут храниться файлы. По-русски buckets - ведра, что для меня звучит странно, так что предпочитаю использовать оригинальное название.

А вот теперь пошли нюансы.

![](https://raw.githubusercontent.com/Erlemar/various-content/master/10.jpg?raw=true)

При создании нужно указать имя, и тут необходимо соблюдать осторожность. Первоначально я использовал название, содержащее дефисы, но никак не мог подключиться к облаку. Оказалось, что некоторые отдельные символы действительно вызывают эту проблему. Есть способы обхода, но они работают не для всех. В итоге я стал использовать вариант названия с нижним подчеркиванием (digit_draw_recognize), с ним проблем не возникало.

Далее нужно указать регион. Можно выбирать практически любой, но при этом стоит руководствоваться вот этой [табличкой](http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region).
![](https://raw.githubusercontent.com/Erlemar/various-content/master/11.jpg?raw=true)

Во-первых, нам понадобится Endpoint для выбранного региона, во-вторых, проще использовать регионы, поддерживающие 2 и 4 версии подписи, чтобы можно были использовать то, что удобно. Я выбрал US East (N. Virginia).

Остальные параметры при создании bucket можно не менять.

Ещё одним важным моментом является настройка CORS.

![](https://raw.githubusercontent.com/Erlemar/various-content/master/12.jpg?raw=true)

В настройку нужно внести следующий код:

```html
<CORSConfiguration>
	<CORSRule>
		<AllowedOrigin>*</AllowedOrigin>
		<AllowedMethod>GET</AllowedMethod>
		<MaxAgeSeconds>3000</MaxAgeSeconds>
		<AllowedHeader>Authorization</AllowedHeader>
	</CORSRule>
</CORSConfiguration>
```

Про эту настройку можно подробнее прочитать [здесь](http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html) и установить более мягкие/жесткие параметры. Изначально я думал, что нужно указать больше методов (а не только GET), но всё итак работает.

Теперь собственно говоря о коде. Пока речь пойдёт только о том, как сохранять файлы на Amazon. О получении данных с облака будет сказано немного позже.

```python
REGION_HOST = 's3-external-1.amazonaws.com'
conn = S3Connection(os.environ['AWS_ACCESS_KEY_ID'], os.environ['AWS_SECRET_ACCESS_KEY'], host=REGION_HOST)
bucket = conn.get_bucket('testddr')

k = Key(bucket)
fn = 'tmp/' + filename
k.key = filename
k.set_contents_from_filename(fn)
```
Первым делом необходимо создать подключение к облаку. Для этого нужно указать Access Key ID, Secret Access Key и Region Host. Ключи мы уже сгенерили, но указывать их явно - это опасно. Такое можно делать только при тестировании кода локально. Я пару раз коммитил код на github с открыто указанными ключами - их крали в течение пары минут. Не повторяйте моих ошибок. Heroku предоставляет возможность хранить ключи и обращаться к ним по названиям - об этом чуть ниже. Region host - это значение endpoint из таблички с регионами.
После этого нужно подключиться к bucket - здесь уже вполне допустимо указывать имя напрямую.

`Key` используется для работы с объектами в корзине. Для сохранения объекта нужно указать название файла (k.key), а затем вызвать `k.set_contents_from_filename()` с указанием пути к файлу, который нужно сохранить.

Всё, теперь файл сохранен!


### Heroku
Пришло время рассказать о том, как разместить сайт на Heroku. Heroku - облачная платформа, поддерживающая несколько языков, и позволяющая быстро и удобно разворачивать веб-приложения. Есть возможность использовать Postgres и вообще много чего интересного. Вообще говоря, я мог бы хранить изображения, используя ресурсы Heroku, но мне нужно хранить разные типы данных, поэтому проще было использовать отдельное облако.
Heroku предлагает несколько ценовых планов, но для моего приложения (в том числе полноценного, а не этого маленького) вполне хватает бесплатного. Единственный минус - приложение "засыпает" через полчаса активности и при следующем запуске оно может потратить секунд 30 на "просыпание".
В сети можно найти много гайдов по разворачиванию приложений на Heroku (вот ссылка на [официальный](https://devcenter.heroku.com/articles/getting-started-with-python#introduction), но большинство из них использует консоль, а я предпочитаю пользоваться интерфейсами. К тому же это кажется мне значительно проще и удобнее.

Итак, к сути. Для подготовки приложения нужно создать несколько файлов:

* необходима система контроля версий .git. Обычно она создаётся автоматически при создании репозитория, но нужно проверить, что она действительно есть;
* `runtime.txt` - в этом файле должна быть указана необходимая версия используемого языка программирования, в моём случае - python-3.6.1;
* `Procfile` - это файл без расширения. В нём указывается, какие команды должны запускаться на heroku. Здесь определяется тип процесса  `web`, что запускается (мой скрипт), и адрес:
```
web: python main.py runserver 0.0.0.0:5000
```
* `requirements.txt` - список библиотек, которые будут установлены на Heroku. Лучше указывать нужные версии;
* И последнее - в папке "tmp" должен лежать хоть один файл, иначе могут возникнуть проблемы с сохранением файлов в этой папке в процессе работы приложения;

Теперь можно начинать процесс создания приложения. На этот момент должен иметься аккаунт на heroku и подготовленный github-репозиторий.

Со [страницы](https://dashboard.heroku.com/apps) со списком приложений создаётся новое:
![](https://raw.githubusercontent.com/Erlemar/various-content/master/2.jpg?raw=true)

Указываем название и страну:
![](https://raw.githubusercontent.com/Erlemar/various-content/master/3.jpg?raw=true)

На вкладке "Deploy" выбираем подключение к Github:
![](https://raw.githubusercontent.com/Erlemar/various-content/master/4.jpg?raw=true)

Подключаемся и выбираем репозиторий:
![](https://raw.githubusercontent.com/Erlemar/various-content/master/5.jpg?raw=true)

Имеет смысл включить автоматическое обновление приложения на Heroku при каждом обновлении репозитория. И можно начинать развертывание:
![](https://raw.githubusercontent.com/Erlemar/various-content/master/6.jpg?raw=true)

Смотрим логи и надеемся, что всё прошло успешно:
![](https://raw.githubusercontent.com/Erlemar/various-content/master/7.jpg?raw=true)

Но ещё не всё готово, перед первым запуском приложения на вкладке настроек надо задать переменные - ключи для облака Amazon.
![](https://raw.githubusercontent.com/Erlemar/various-content/master/8.jpg?raw=true)

Теперь можно запускать приложение и работать с ним.

## Обработка изображений для моделей

Я сохраняю нарисованные изображения как картинки в оригинальном формате, чтобы всегда можно было посмотреть их и попробовать разные варианты обработки.
Вот несколько примеров:
![](https://raw.githubusercontent.com/Erlemar/various-content/master/13.jpg?raw=true)


Идея обработки изображений для моего проекта следующая (похожая на mnist): нарисованная цифра масштабируется так, чтобы умещаться в квадрат 20х20, сохраняя пропорции, а затем помещается в центр белого квадрата размерами 28х28. Для этого необходимы следующие шаги:

* найти границы изображения (граница имеет форму прямоугольника);
* найти высоту и ширину ограниченного прямоугольника;
* приравнять большую сторону к 20, а меньшую масштабировать так, чтобы сохранить пропорции;
* найти точку старта для рисования отмасштабированной цифры на квадрате 28х28 и нарисовать её;
* конвертировать в np.array, чтобы с данными можно было работать, и нормализовать;

```python
# Считывание из файла
img = Image.open('tmp/' + filename)
# Нахождение границы
bbox = Image.eval(img, lambda px: 255-px).getbbox()
if bbox == None:
	return None
# Оригинальные длины сторон
widthlen = bbox[2] - bbox[0]
heightlen = bbox[3] - bbox[1]

# Новые
if heightlen > widthlen:
	widthlen = int(20.0 * widthlen/heightlen)
	heightlen = 20
else:
	heightlen = int(20.0 * widthlen/heightlen)
	widthlen = 20

# Стартовая точка рисования
hstart = int((28 - heightlen) / 2)
wstart = int((28 - widthlen) / 2)

# Отмасштабированная картинка
img_temp = img.crop(bbox).resize((widthlen, heightlen), Image.NEAREST)

# Перенос на белый фон с центрированием
new_img = Image.new('L', (28,28), 255)
new_img.paste(img_temp, (wstart, hstart))

# Конвертация в np.array и нормализация
imgdata = list(new_img.getdata())
img_array = np.array([(255.0 - x) / 255.0 for x in imgdata])
```

## Аугментация изображений

Очевидно, что 1000 изображений - маловато для тренировки нейронной сети, а ведь часть из них нужно использовать для валидации моделей. Решением этой проблемы является аугментация данных - создание дополнительных изображений, которые помогут увеличить размер датасета. Вариантов аугментации много, но стоит использовать лишь те, которые релевантны для поставленной задачи. Я решил использовать масштабирование и повороты.
Люди могут рисовать цифры шире или уже, с большей или меньшей длиной. При изначальной обработке изображения масштабируются в квадрат 20х20 и одна сторона будет меньше другой. Я решил создавать 4 разных масштабирования - 4 варианта комбинаций двух длин сторон.
Кроме того цифры могут рисоваться с разным наклоном. Я решил, что максимальный возможный наклон - 30 градусов в ту или иную сторону; при использовании шага в 5 градусов, получается 12 разных вариантов. Чтобы было не слишком много изображений, из этих 12 случайным образом выбираются 6.

```python
image = Image.open('tmp/' + filename)
		
ims_add = []
labs_add = []
angles = np.arange(-30, 30, 5)
bbox = Image.eval(image, lambda px: 255-px).getbbox()

widthlen = bbox[2] - bbox[0]
heightlen = bbox[3] - bbox[1]

if heightlen > widthlen:
	widthlen = int(20.0 * widthlen/heightlen)
	heightlen = 20
else:
	heightlen = int(20.0 * widthlen/heightlen)
	widthlen = 20

hstart = int((28 - heightlen) / 2)
wstart = int((28 - widthlen) / 2)

for i in [min(widthlen, heightlen), max(widthlen, heightlen)]:
	for j in [min(widthlen, heightlen), max(widthlen, heightlen)]:
		resized_img = image.crop(bbox).resize((i, j), Image.NEAREST)
		resized_image = Image.new('L', (28,28), 255)
		resized_image.paste(resized_img, (wstart, hstart))

		angles_ = random.sample(set(angles), 6)
		for angle in angles_:
			transformed_image = transform.rotate(np.array(resized_image), angle, cval=255, preserve_range=True).astype(np.uint8)
			labs_add.append(int(label))
			img_temp = Image.fromarray(np.uint8(transformed_image))
			imgdata = list(img_temp.getdata())
			normalized_img = [(255.0 - x) / 255.0 for x in imgdata]
			ims_add.append(normalized_img)
```

В итоге получилось 800 * 24 = 19200 изображений в тренировочном датасете и 200 осталось для валидации. Теперь можно выбирать архитектуру нейронных сетей и тренировать их.

## FNN

Первой из сетей является обычная feed forward neural net. Я использовал структуру, предложенную в cs231n, и она успешно сработала. 

![](https://digits-draw-recognize.herokuapp.com/static/9.jpg)

Код для создания картинки под катом.
https://gist.github.com/Erlemar/703c094ad9484ae89a17ff5f60fc5ea3

Код для самой сети, (под катом)
https://gist.github.com/Erlemar/cb431b88d1d64e163601c68a530b8e9c#file-fnn-ipynb

На вход подаются обработанные изображения, далее идёт скрытый слой, в котором 100 нейронов. В качестве функции активации скрытого слоя используется ReLU. На выходе имеем 10 нейронов с активацией softmax. Softmax даёт вероятность принадлежности к тому или иному классу. 

Код работает следующим образом:

* Для задания начальных значений весов используется инициализация Ксавьера (Xavier). При таком варианте инициализации дисперсия берётся равной 2 / количество входящих нейронов. Благодаря этому веса получаются не слишком большими и не слишком маленькими;
* В методе loss реализован шаг градиентного спуска и расчёт лосса с учетом L2 регуляризации;
* В методе train осуществляется итеративная тренировка на мини-батчах. Лосс считается после каждого шага. После каждой эпохи считается точность на тренировочном и валидационном датасетах, а также уменьшается learning rate;
* Метод predict_single даёт предсказания для одного изображения, а predict - для нескольких;

При начале работе с сетью необходимо задать размеры всех трёх слоёв:

```python
input_size = 28 * 28
hidden_size = 100
num_classes = 10
net = tln(input_size, hidden_size, num_classes)
```

При тренировке необходимо указать большее количество параметров и данных:
* тренировочные и валидационные данные и их лейблы;
* количество итераций и размер мини-батча, их частное будет количеством эпох;
* изначальное значение learning rate и размер decay, на которое умножается learning rate после каждой эпохи;
* размер коэффициента регуляризации;
* и значение verbose для отображения/скрытия процесса тренировки;

```python
stats = net.train(X_train_, y_train_, X_val, y_val,
            num_iters=19200, batch_size=24,
            learning_rate=0.1, learning_rate_decay=0.95,
            reg=0.001, verbose=True)
```

Благодаря тому, что тренировка возвращает историю изменения loss и точности на тренировочных и валидационных данных, строить графики довольно просто:

```python
plt.subplot(2, 1, 1)
plt.plot(stats['loss_history'])
plt.title('Loss history')
plt.xlabel('Iteration')
plt.ylabel('Loss')

plt.subplot(2, 1, 2)
plt.plot(stats['train_acc_history'], label='train')
plt.plot(stats['val_acc_history'], label='val')
plt.title('Classification accuracy history')
plt.xlabel('Epoch')
plt.ylabel('Clasification accuracy')
plt.legend()
plt.show()
```
![](https://digits-draw-recognize.herokuapp.com/static/4.jpg)


## Тренировка и подбор параметров
Выбор оптимальных параметров - больная тема. Переобучение, недообучение, предсказание одного и того же класса для разных данных - проблем, которые могут возникнуть при тренировке сети полно. Соответственно возникает вопрос подбора хороших параметров. Здесь я сразу решил, что не гонюсь за максимальной точностью, и разница в несколько процентов не является критичной при построении модели. Напомню, что важной частью проекта является дотренировка моделей, а это значит, что точность всё равно будет изменяться.

Тем не менее всё же хочется получить приличную точность, а значит надо приложить усилия. Я решил идти методом простого перебора параметров. Для этого на глаз были определены возможные значения learning rate, regularization, количества нейронов в скрытом слое, количества эпох и размера мини-батча; для каждой комбинации сеть тренировалась, и запоминались точности на тренировочном и валидационном датасетах. Модель с наивысшей точностью считалась лучшей.

```python
best_net = None
results = {}
best_val = -1
learning_rates = [0.001, 0.1, 0.01, 0.5]
regularization_strengths = [0.001, 0.1, 0.01]
hidden_size = [5, 10, 20]
epochs = [1000, 1500, 2000]
batch_sizes = [24, 12]

best_params = 0
for l in learning_rates:
    for r in regularization_strengths:
        for h in hidden_size:
            for e in epochs:
                for b in batch_sizes:
                    print('learning rate: {0}, regularization strength: {1}, hidden size: {2}, iterations: {3}'.format(l, r, h, e))
                    net = TwoLayerNet(input_size, h, num_classes)
                    stats = net.train(X_train_, y_train, X_val_, y_val,
                        num_iters=e, batch_size=b,
                        learning_rate=l, learning_rate_decay=0.95,
                        reg=r, verbose=False)
                    y_train_pred = net.predict(X_train_)
                    train_acc = np.mean(y_train == y_train_pred)

                    val_acc = (net.predict(X_val_) == y_val).mean()
                    print('Validation accuracy: ', val_acc)
                    results[(l, r, h, e)] = val_acc
                    if val_acc > best_val:
                        best_val = val_acc
                        best_net = net   
                        best_params = (l, r, h, e)
```

Точность наилучшей модели была хорошей, казалось бы всё неплохо, но я чувствовал, что что-то не так. Дело в том, что количество итераций было небольшим, learning rate оставалась довольно высокой и точность модели была весьма нестабильной. Я решил привнести в параметры модели некоторую логику. Размер мини-батча определялся на глаз, но ведь это не совсем корректно в моём случае: если я планирую дообучение, то модель должна уметь тренироваться на 1 новом изображении. 1 изображение означает 24 аугментированных, исходя из этого было решено попробовать сделать размер мини-батча таким же. Вместе с этим я увеличил количество эпох до 24.
Кроме того мне захотелось узнать точность предсказаний моей модели на MNIST, так что я добавил в код сохранение результатов предсказания на MNIST.
Код для считывания и подготовки данных MNIST:

```python
def read(path = "."):

    fname_img = os.path.join(path, 't10k-images.idx3-ubyte')
    fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')

    with open(fname_lbl, 'rb') as flbl:
        magic, num = struct.unpack(">II", flbl.read(8))
        lbl = np.fromfile(flbl, dtype=np.int8)

    with open(fname_img, 'rb') as fimg:
        magic, num, rows, cols = struct.unpack(">IIII", fimg.read(16))
        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)

    get_img = lambda idx: (lbl[idx], img[idx])

    for i in range(len(lbl)):
        yield get_img(i)
	
test = list(read(path="/MNIST_data"))
mnist_labels = np.array([i[0] for i in test]).astype(int)
mnist_picts = np.array([i[1] for i in test]).reshape(10000, 784) / 255
y_mnist = np.array(mnist_labels).astype(int)
```

В итоге получился такой вот процесс тренировки:

![](https://raw.githubusercontent.com/Erlemar/various-content/master/14.jpg?raw=true)

По графику видно, что можно было бы ограничиться ~10 эпохами, но я решил потренировать модель подольше, чтобы learning rate уменьшилась.

Думаю, что сразу заметно, что точность на MNIST довольно низка. Я было забеспокоился, но подумал о том, что наверняка собранные мной данные и данные MNIST могут различаться. Чтобы проверить это я взял архитектуру CNN из официального [гайда](https://www.tensorflow.org/get_started/mnist/pros) Tensorflow для MNIST и натренировал её. На тестовой части MNIST точность была 99%+ (как и ожидалось), а вот на моих данных точность оказалась значительно ниже **тут точность**. Исходя из этого я решил, что мои данные и MNIST действительно различаются, а моя модель достаточно хороша и не стоит пытаться сделать её лучше.

Впрочем, я всё же попытался это сделать:

* пробовал добавить скрытые слои;
* пробовал значительно увеличить количество итераций/эпох;
* пробовал разные варианты оптимизации: Нестерова, Адама, rmsprop;

Ничего из этого не смогло значительно увеличить точность модели. Наверное, из имеющихся данных было проблематично извлечь больше информации. Пришло время перейти к следующему этапу.

## Дообучение FNN

Реализация возможности дообучения оказалась до удивления простой. Изменения в коде были следующими:

* Веса не инициализируются случайным образом, а передаются в модель. Соответственно должно быть реализовано сохранение и загрузка весов;
* Лосс не считается, поскольку не нужен;
* Модель делает один шаг градиентного спуска на получаемых данных (1 изображение, аугментированное до 24);
* Все параметры модели (learning rate и другие) фиксированы и заданы прямо в модели;
* Метод predict-single возвращает 3 наиболее вероятных предсказания и их вероятности. Это относится не к самой дотренировке, а к работе кода в целом;

Посмотреть актуальный код можно [здесь](https://github.com/Erlemar/digit-draw-recognize/blob/master/two_layer_net.py).

Я потратил некоторое время на проверку работоспособности этих изменений, на проверку точности - не станет ли она изменяться слишком резко, а также на выбор learning rate.
Выбор learning rate был самым сложным. Изначальное значение было 0.1, а после 24 эпох 0.1 x (0.95 ^ 24). Проблема в том, что это значение использовалось для всей эпохи - 800 итераций. Если использовать это значение, то каждое новое изображение может значительно изменять веса. Я пробовал много разных значений и остановился на 0.1 x (0.95 ^ 24) / 32. В итоге одно изображение не сможет значительно изменить веса, но несколько изображений постепенно приведут к заметным изменениям.

## CNN

Вообще говоря, после создания FNN я занялся созданием рабочей версии сайта, но после того как я сделал CNN сайт пришлось немного модифицировать, так что значительно удобнее поговорить вначале о CNN, а потом уже о работе над сайтом.
При создании архитектуры CNN я опирался на cs231n и вариант из гайда Tenforlow для MNIST, но в итоге модифицировал под свои нужды.

![](https://digits-draw-recognize.herokuapp.com/static/10.jpg)
Код для создания картинки под катом:
https://gist.github.com/Erlemar/703c094ad9484ae89a17ff5f60fc5ea3

Первые 2 слоя имеют одинаковую структуру: convolutional 4x4, ReLU, max pooling 2x2, dropout. После этого идёт полносвязный слой с ReLU, dropout и ещё один полносвязный слой.
Поскольку используется Tensorflow, код имеет структуру, значительно отличающуюся от FNN на numpy
Код под катом:
https://gist.github.com/Erlemar/9ca303ed987acb85d5bb04c6386ad879#file-cnn-ipynb

Код работает следующим образом:

* Вначале задаётся структура нейронной сети, для удобства это делается в отдельной функции;
* Затем задаются параметры инициализации весов: нули для биасов и инициализация Ксавьера для самих весов;
* X, y и значения dropout являются входными параметрами;
* Cost считается как softmax_cross_entropy_with_logits с регуляризацией L2;
* Оптимизатор - RMSPropOptimizer;
* Я прямо задаю количество эпох (256), при желании можно это количество изменить или реализовать раннюю остановку;
* При каждой эпохе модель тренируется на тренировочных данных с dropout 0.2 для convolutional слоёв и 0.5 для полносвязных. Кроме того считаются точность и лосс для тренировочных, валидационных и "тестовых" (MNIST) данных;
* По завершению тренировки модель сохраняется в файл, чтобы её можно загрузить и использовать для предсказаний/тренировки;

Загрузка MNIST для TF реализована очень удобно:
```python
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
mnist_test_images = mnist.test.images.reshape(-1, 28, 28, 1)
```
Тренировочные и валидационные данные подготавливаются также, как и для FNN, лишь с небольшим нюансом из-за того, что модель предлагает другие измерения у данных:

```python
trX = X_train.reshape(-1, 28, 28, 1) # 28x28x1
teX = X_val.reshape(-1, 28, 28, 1)
enc = OneHotEncoder()
enc.fit(y.reshape(-1, 1), 10).toarray() # 10x1
trY = enc.fit_transform(y_train.reshape(-1, 1)).toarray()
teY = enc.fit_transform(y_val.reshape(-1, 1)).toarray()
```

## Тренировка и подбор параметров

Параметры CNN такие же как и в FNN, просто задаются немного по-другому. Из-за того, что CNN тренируется дольше, я пробовал меньше комбинаций параметров. Некоторые из комбинаций давали нестабильные решения, например такие:

![](https://digits-draw-recognize.herokuapp.com/static/6.jpg)

Оптимальный же вариант был таким:
![](https://digits-draw-recognize.herokuapp.com/static/7.jpg)

Очевидно, что нет смысла тренировать модель так долго, поэтому я перезапустил тренировку и остановился на ~100 эпохах.

## Дообучение

Изменения в коде для дотренировки CNN были примерно такими же, как и для дотренировки FNN: передача весов в модель, совершается 2 шаг, метод предсказания возвращает топ-3 предсказания и их вероятности.

Посмотреть актуальный код можно [здесь](https://github.com/Erlemar/digit-draw-recognize/blob/master/conv_net.py).

Некоторое время опять же ушло на подбор learning rate. В итоге я остановился на 0.00001, что в 100 раз меньше оригинального значения. Впрочем, проверить успешность этого значения было проблематично. Надеюсь, что время покажет корректность или ошибку такого решения.

## Bootstrap и дизайн сайта

Наконец пришло время





**!!!!!!!!!!!!!!!!!!! проверить код для запуска приложения на heroku - gunicorn **



## Всякие улучшения.

Touch, MNIST, чистка кода (и облом с переменными в tf). Страницы с описанием проекта.



# Запуск проекта



## Слежение за точность.



# Заключение



## Существующие проблемы



## Планы на следующий проект



## Общее впечатление от проекта
